{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.doer import Coextractor\n",
    "from model.feature_extractor import FeatureExtractor\n",
    "from config import Config\n",
    "from utils import load_data, prep_train_data, load_lexicon\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, None, 400)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "first_ate_rnn (Bidirectional)   (None, None, 600)    1321800     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "first_asc_rnn (Bidirectional)   (None, None, 600)    1321800     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "first_ate_dropout (Dropout)     (None, None, 600)    0           first_ate_rnn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "first_asc_dropout (Dropout)     (None, None, 600)    0           first_asc_rnn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cross_shared_unit (CrossSharedU [(None, 40, 600), (N 720002      first_ate_dropout[0][0]          \n",
      "                                                                 first_asc_dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 40, 600)      0           cross_shared_unit[0][0]          \n",
      "                                                                 cross_shared_unit[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 40, 600)      0           cross_shared_unit[0][0]          \n",
      "                                                                 cross_shared_unit[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "second_ate_rnn (Bidirectional)  (None, 40, 600)      1801800     lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "second_asc_rnn (Bidirectional)  (None, 40, 600)      1801800     lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "second_ate_dropout (Dropout)    (None, 40, 600)      0           second_ate_rnn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "second_asc_dropout (Dropout)    (None, 40, 600)      0           second_asc_rnn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_ate (Lambda)           (None, 600)          0           first_ate_rnn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_asc (Lambda)           (None, 600)          0           first_asc_rnn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ate_output (Dense)              (None, 40, 3)        1803        second_ate_dropout[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "ste_output (Dense)              (None, 40, 3)        1803        second_ate_dropout[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "asc_output (Dense)              (None, 40, 3)        1803        second_asc_dropout[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "sentiment_lexicon_enhancement ( (None, None, 3)      1803        first_asc_dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "aspect_term_length_enhancement  (None, 1)            601         max_pool_ate[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "aspect_polarity_length_enhancem (None, 1)            601         max_pool_asc[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 6,975,616\n",
      "Trainable params: 6,975,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Aspect and Sentiment Term Extraction\n",
      "Precision :  0.919102876078572\n",
      "Recall :  0.9172997932214912\n",
      "F1-score :  0.9180616525324259\n",
      "Coextraction f1/acc :  0.717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9180616525324259, 0.9239847970848517]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "clear_session()\n",
    "\n",
    "train_data = 'dataset/train_4k.txt'\n",
    "test_data = 'dataset/test_1k.txt'\n",
    "mpqa_lexicon_data = 'dataset/annotated/mpqa_lexicon.txt'\n",
    "general_embedding_model = '../word_embedding/general_embedding/general_embedding_300.model'\n",
    "domain_embedding_model = '../word_embedding/domain_embedding/domain_embedding_100.model'\n",
    "config = Config()\n",
    "config.mpqa_lexicon = load_lexicon(mpqa_lexicon_data)\n",
    "\n",
    "X, y = load_data(train_data)\n",
    "X_test, y_test = load_data(test_data)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "sentences = X_val\n",
    "\n",
    "feature_extractor = FeatureExtractor(general_embedding_model, domain_embedding_model, general_dim=config.dim_general, domain_dim=config.dim_domain)\n",
    "\n",
    "X_train, y_train = prep_train_data(X_train, y_train, feature_extractor, feature='double_embedding', config=config)\n",
    "\n",
    "X_test = feature_extractor.get_features(X_test, max_len=config.max_sentence_size)\n",
    "X_val, y_val2 = prep_train_data(X_val, y_val, feature_extractor, feature='double_embedding', config=config)\n",
    "\n",
    "coextractor = Coextractor(config)\n",
    "coextractor.load(\"saved_models/P3_Diff/P3_300_1_0.5_weights\")\n",
    "print(coextractor.model.summary())\n",
    "\n",
    "coextractor.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('ac nya', 'tidak dingin.', 'NG'), ('kamar', 'sempit', 'NG')]]\n",
      "[[('pelayanan', 'ramah,', 'PO'), ('kamar', 'bersih,', 'PO'), ('kasur', 'empuk.', 'PO'), ('kamar mandi nya', 'empuk.', 'NG')]]\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = 'ac nya tidak dingin. kamar sempit tapi wajar untuk harga segitu'\n",
    "sample_sentence2 = 'pelayanan ramah, kamar bersih, kasur empuk. tetapi kamar mandi nya kotor'\n",
    "print(coextractor.predict_one(sample_sentence, feature_extractor))\n",
    "print(coextractor.predict_one(sample_sentence2, feature_extractor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
